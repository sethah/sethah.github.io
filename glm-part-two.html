
<!DOCTYPE html>
<html lang="en">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>

    <link rel="stylesheet" type="text/css" href="https://sethah.github.io/theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="https://sethah.github.io/theme/pygments/github.min.css">
  <link rel="stylesheet" type="text/css" href="https://sethah.github.io/theme/font-awesome/css/font-awesome.min.css">


    <link href="https://sethah.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Seth's Blog Atom">



  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />


<meta name="author" content="Seth" />
<meta name="description" content="Generalizing the linear model¶Sure, everybody knows what linear regression is (unless they are seriously uncool), but only the most hip among us know that a linear regression is just a Generalized Linear Model (GLM) with a Gaussian family and an identity link function. Say wut? Yes, GLMs are the old, unpopular parents who spawned famous children like linear and logistic regression. GLMs are generalized, which means that they are far less specific than a linear regression and far more adaptable to different types of prediction problems. A consequence of this, as we'll soon find out, is that there are a LOT of symbols, the notation is heavy, and shit gets crazy real fast." />
<meta name="keywords" content="ml, python">
<meta property="og:site_name" content="Seth's Blog"/>
<meta property="og:title" content="GLMs Part II - Exponential families and the GLM log-likelihood"/>
<meta property="og:description" content="Generalizing the linear model¶Sure, everybody knows what linear regression is (unless they are seriously uncool), but only the most hip among us know that a linear regression is just a Generalized Linear Model (GLM) with a Gaussian family and an identity link function. Say wut? Yes, GLMs are the old, unpopular parents who spawned famous children like linear and logistic regression. GLMs are generalized, which means that they are far less specific than a linear regression and far more adaptable to different types of prediction problems. A consequence of this, as we'll soon find out, is that there are a LOT of symbols, the notation is heavy, and shit gets crazy real fast."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="https://sethah.github.io/glm-part-two.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2017-07-06 00:00:00-07:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="https://sethah.github.io/author/seth.html">
<meta property="article:section" content="posts"/>
<meta property="article:tag" content="ml"/>
<meta property="article:tag" content="python"/>
<meta property="og:image" content="/images/profile.jpg">

  <title>Seth's Blog &ndash; GLMs Part II - Exponential families and the GLM log-likelihood</title>

</head>
<body>
  <aside>
    <div>
      <a href="https://sethah.github.io">
        <img src="/images/profile.jpg" alt="" title="">
      </a>
      <h1><a href="https://sethah.github.io"></a></h1>


      <nav>
        <ul class="list">
          <li><a href="https://sethah.github.io/pages/about.html#about">About</a></li>

        </ul>
      </nav>

      <ul class="social">
        <li><a class="sc-linkedin" href="https://linkedin.com/in/sethah" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        <li><a class="sc-github" href="https://github.com/sethah" target="_blank"><i class="fa fa-github"></i></a></li>
        <li><a class="sc-twitter" href="https://twitter.com/shendrickson16" target="_blank"><i class="fa fa-twitter"></i></a></li>
      </ul>
    </div>


  </aside>
  <main>

    <nav>
      <a href="https://sethah.github.io">    Home
</a>

      <a href="/archives.html">Archives</a>
      <a href="/categories.html">Categories</a>
      <a href="/tags.html">Tags</a>

      <a href="https://sethah.github.io/feeds/all.atom.xml">    Atom
</a>

    </nav>

<article class="single">
  <header>
    <h1 id="glm-part-two">GLMs Part II - Exponential families and the GLM log-likelihood</h1>
    <p>
          Posted on Thu 06 July 2017 in <a href="https://sethah.github.io/category/posts.html">posts</a>


    </p>
  </header>


  <div>
    <style type="text/css">/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI colors. */
.ansibold {
  font-weight: bold;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  border-left-width: 1px;
  padding-left: 5px;
  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
}
div.cell.jupyter-soft-selected {
  border-left-color: #90CAF9;
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected {
  border-color: #ababab;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
}
@media print {
  div.cell.selected {
    border-color: transparent;
  }
}
div.cell.selected.jupyter-soft-selected {
  border-left-width: 0;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  padding: 0.4em;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
  /* .CodeMirror-lines */
  padding: 0;
  border: 0;
  border-radius: 0;
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}


.rendered_html pre,



.rendered_html tr,
.rendered_html th,

.rendered_html td,


.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,

div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
</style>
<style type="text/css">.highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */</style>
<style type="text/css">
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }
</style>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Generalizing-the-linear-model">Generalizing the linear model<a class="anchor-link" href="#Generalizing-the-linear-model">&#182;</a></h2><p>Sure, everybody knows what linear regression is (unless they are <em>seriously</em> uncool), but only the most hip among us know that a linear regression is just a <a href="https://en.wikipedia.org/wiki/Generalized_linear_model">Generalized Linear Model (GLM)</a> with a Gaussian family and an identity link function. Say wut? Yes, GLMs are the old, unpopular parents who spawned famous children like linear and logistic regression. GLMs are generalized, which means that they are far less specific than a linear regression and far more adaptable to different types of prediction problems. A consequence of this, as we'll soon find out, is that there are a LOT of symbols, the notation is heavy, and shit gets crazy real fast.</p>
<p>But GLMs are important, ok? They're not as cool as deep learning. Hell, they're not even as cool as decision trees! But they work well, they've been studied and in use for a long time, and they're fairly easy to interpret. Linear models are the "Hello World!" of machine learning. They also make for great job interview questions - don't ever trust a data scientist that can't give a sound, thorough explanation of a linear model!</p>
<p>With that, we'll begin the dirty work of trying to understand these archaic beasts. The notation is heavy and there are going to be lots of greek letters along the way, but we'll take it slow, add some intuition along the way, and maybe drink a beer (wine also acceptable) while we do it.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Formulating-the-problem">Formulating the problem<a class="anchor-link" href="#Formulating-the-problem">&#182;</a></h2><p>I always find it helpful to state what it is we're trying to accomplish, at a high level first, and then try to drill down from there. To start, we have some data $X$ that influences some outcome, some <em>result</em> $y$, in some way. Going one step further, we can say that our output $y$ is produced by our data $X$ through <em>some process</em>.</p>
<blockquote><p>We want to use the data to find the process by which the data generates the outcome.</p>
</blockquote>
<p><img src="../../images/GLM2/GLM2_1.jpg" width="400"></p>
<p>For clarity, let's use an example. We have some data on a house, say the size and number of bedrooms, and we know the price of the house. The data is known and fixed and the price is a result of the data being what it is. There is some process by which the price of a house is produced from the size and number of bedrooms of that house (a coarse model, to be sure). Say a house is 1000 square feet and has 2 bedrooms - that is our data. Let's say the price of the house is 1.5 million (it's in Palo Alto, people!) - that is our outcome. That process, in real life, might be that a seller decides that their 1000 sq ft and 2 bedroom house should go for 1.4 million, which we'll call the listing price. But due to some other unpredictable circumstances, the house doesn't sell for exactly 1.4 million. There is a bit of "randomness" in the final price of each house relative to the initial listing price.</p>
<p>We have now actually loosely specified a model by which the data produces the outcome:</p>
<ol>
<li>Use the data to find a reasonable expected value for the outcome.</li>
<li>Inject randomness.</li>
</ol>
<p>We can state this more generically. First, injecting that randomness is the same as saying that our outcome $Y$ is a random variable that is drawn from a probability distribution. For that house with 2 bedrooms and 1000 square feet, we would say that the price is drawn from <em>some</em> distribution that has a mean of 1.4 million. The shape and width of that distribution must be determined - we'll use some data for that! Let's restate our goal:</p>
<blockquote><p>We want to use our data to find the probability distribution that the random variable $Y$ is drawn from.</p>
</blockquote>
<p><img src="../../images/GLM2/GLM2_2.jpg" width="400"></p>
<p>This is still pretty vague! A couple of things will help us simplify this. First, since we're dealing with generalized <em>linear</em> models here, we know we want to use linear combinations of the data.</p>
<blockquote><p>We want to use a linear combination of our data to find the probability distribution that the random variable $Y$ is drawn from.</p>
</blockquote>
<p><img src="../../images/GLM2/GLM2_3.jpg" width="400"></p>
<p>Second, we assume that the outcomes are all drawn from the same type of distribution. For example, we might assume all of the outcomes are drawn from a Gaussian distribution or a Poisson distribution or a Binomial distribution. The type of distribution is a modeling choice and is selected beforehand. In a GLM, we use only specific types of probability distributions that can be fully specified by a finite number of distribution parameters. This assumption means that "finding the probability distribution that the random variable $Y$ is drawn from" actually means "finding the <strong>parameters</strong> of the probability distribution that the random variable $Y$ is drawn from." Ok, so now we restate:</p>
<blockquote><p>We want to use a linear combination of our data to find the parameters of the probability distribution that the random variable $Y$ is drawn from.</p>
</blockquote>
<p><img src="../../images/GLM2/GLM2_4.jpg" width="400"></p>
<p>For GLMs, it is possible to drill down even further because of yet another assumption. In a GLM, we limit ourselves to only specific types of parameterizable distributions - distributions from the <a href="http://web.as.uky.edu/statistics/users/pbreheny/760/S13/notes/1-31.pdf"><em>exponential family</em></a>. We will find out, in short time, all about this family and the mathematics that come with it. But for now, we'll just restate our problem to incorporate this knowledge:</p>
<blockquote><p>We want to use a linear combination of our data to find the parameters of the exponential family distribution that the random variable $Y$ is drawn from.</p>
</blockquote>
<p><img src="../../images/GLM2/GLM2_5.jpg" width="400"></p>
<p>We still have a problem, that we don't know and cannot ever truly know, the parameters of the exponential family distribution that outcome is drawn from. In fact, it is unlikely that the outcome was even generated this way! So, what we are truly after is to find, given our assumptions about how the data was generated, the parameters of the exponential family distribution that makes the data most <em>likely</em> to have occurred. This is called maximum likelihood estimation and was introduced in <a href="/glm-part-i">the first post of this series</a>.</p>
<blockquote><p>We want to use a linear combination of our data to find the parameters of the exponential family distribution that maximize the likelihood of observing the outcome data in the training set.</p>
</blockquote>
<p>And that, my friends, is really it. That's the problem that a GLM aims to solve - given some distribution from the exponential family, what is the best way to relate a linear combination of the data to the parameters of that distribution, in order to maximize some <em>likelihood</em> function?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Exponential-families">Exponential families<a class="anchor-link" href="#Exponential-families">&#182;</a></h2><p>There is surely a whole lot of literature on exponential family distributions that will indulge those who seek a formal treatment of them. Here, I will try to stick to a more intuitive approach.</p>
<p>Exponential family distributions are probability distributions that obey a specific form. All distributions that come from this family - Gaussian, Poisson, Binomial, Gamma, etc... - share <em>nice</em> mathematical properties that, in the case of GLMs, will be rather important. Without further ado, exponential families are probability distributions of the form:</p>
<p>$$
f\left(y|\theta, \phi, w\right) = e^{\frac{y\theta - b(\theta)}{\phi/w} - c(y, \phi)}\\
Y_i \sim f\left(\cdot|\theta_i, \phi, w_i\right)
$$</p>
<p>I warned you about the symbols, didn't I? Let's not get too worked up before we know just how bad this really is. Let's break this down:</p>
<ul>
<li>$w_i$ are known weights, usually 1</li>
<li>$\phi$ is a constant scale parameter that is the same for all $Y_i$</li>
<li>$\theta_i$ is a canonical parameter, the parameter of interest, that is different for each sample</li>
</ul>
<p>Ok, so the weights are known beforehand and are usually equal to one. We shouldn't worry too much about the weights then. $\phi$ is a constant - that makes things much simpler. Finally, $\theta_i$ is the parameter of interest, so that's what we'll worry about. $\theta_i$ is the parameter we're talking about above when we stated that "We want to use a linear combination of our data to find the parameters of the exponential family distribution that the random variable $y$ is drawn from."</p>
<h3 id="Some-common-exponential-family-distributions">Some common exponential family distributions<a class="anchor-link" href="#Some-common-exponential-family-distributions">&#182;</a></h3><p>Before we go too much further, let's take a quick moment to list some example distributions that fit the form of the density function shown above:</p>
<ul>
<li>Gaussian (normal)</li>
<li>Bernoulli</li>
<li>Poisson</li>
<li>Gamma</li>
<li>Chi-square</li>
<li>Dirichlet</li>
</ul>
<p>Now, I know what you're probably thinking... "I know the density functions for some of those distributions and they sure do not fit that $e^{\text{stuff}}$ format!" Well, fortunately or unfortunately, that is incorrect. Yes, even the Bernoulli distribution $p^k (1-p)^{1-k}$ can be stuffed into that format shown above. <a href="http://web.as.uky.edu/statistics/users/pbreheny/760/S13/notes/1-31.pdf">Check it out here</a></p>
<h3 id="How-does-the-exponential-family-fit-into-the-GLM-problem?">How does the exponential family fit into the GLM problem?<a class="anchor-link" href="#How-does-the-exponential-family-fit-into-the-GLM-problem?">&#182;</a></h3><p>We said above that $\theta_i$ is the important part, so let's talk more about it. In an exponential family distribution, we have the relation:</p>
<p>$$
\theta_i = h(\mu_i)\\
\mu_i = E\left[Y_i\right]
$$</p>
<p>This says that the parameter that defines our distribution $\theta_i$ is related to the expected value of the outcome through some function $h(\mu)$. This function is known and is defined by the specific exponential family distribution. For example, in a normal distribution $h(\mu) = \mu$ and in a Poisson distribution $h(\mu) = ln(\mu)$.</p>
<p>Now that we know $\theta_i$ is related to $\mu_i$, we can relate this back to our original goal. We know we are seeking to relate a linear combination of the input data to the parameter that defines our distribution. Specifically, we want relate $X\vec{\beta}$ to $\vec{\theta}$; but since we know that $\vec{\theta}$ is just some function of $\mu$, we can restate this by saying we want to relate $X\vec{\beta}$ to $\vec{\mu}$. For most of this post, we will use a variable $\vec{\eta} = X\vec{\beta}$ which is sometime called the "linear predictor." With that in mind, we say finally that our task now is to relate the linear predictor, $\eta$, to the expected value of the outcome, $\vec{\mu}$.</p>
<p>Intuitively, we are asking "how does the expected value of $Y$ change as the data changes linearly?" For an ordinary least squares model, we say that $E\left[Y\right]$ varies identically with $\vec{\eta}$. For a logistic regression, we need to restrict $E\left[Y\right]$ to lie in the interval [0, 1], so we cannot say the same. Instead, the sigmoid function is used to restrict $E\left[Y\right]$ to [0, 1]. If $Y$ were a count variable, we would want to restrict $Y$ to be positive.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-link-function">The link function<a class="anchor-link" href="#The-link-function">&#182;</a></h2><p>In order to relate the linear predictor to the expected value of the outcome, we use what is called a <em>link function</em>. The link function is a model choice made by the practitioner and depends on the nature of the outcome variable. Formally, the link function $g(\mu)$ is defied as:</p>
<p>$$
g(\mu_i) = \eta_i
$$</p>
<p>Now, recall that the distribution parameter $\theta_i$ is related to the expected value $\mu_i$ through the function $\theta_i = h(\mu_i)$. This means that we can now relate the distribution parameter to the linear predictor:</p>
<p>$$
\theta_i = h(\mu_i)\\
\mu_i = g^{-1}(\eta_i)\\
\theta_i = h(g^{-1}(\eta_i))
$$</p>
<p>Remember that we are trying to find the parameter $\vec{\theta}$ that makes our data most likely to have occurred. This is done by finding a "likelihood" equation that depends on $\vec{\theta}$ and then finding the value of $\vec{\theta}$ that maximizes the likelihood. Since $\vec{\theta}$ depends on $\vec{\eta}$, we can reformulate this problem by finding the values of $\vec{\eta}$ that maximize the likelihood. Finally, since $\vec{\eta} = X\vec{\beta}$, then we can find the regression coefficients that give us the maximum likelihood.</p>
<p>We have effectively stated how $\vec{\theta}$ depends on $\vec{\beta}$ and so we are ready to find the likelihood equation and solve it for the best value of $\vec{\beta}$.</p>
<p><strong>Note:</strong> It is often the case that the link function is chosen such that $g(\mu) = h(\mu)$. If this is the case, then we say that $g(\mu)$ is the <em>canonical link function</em> and much of the math simplifies nicely. In particular, we have that $\theta_i = h(g^{-1}(\eta_i)) = \eta_i$. Many texts and papers present the derivations for GLMs assuming the canonical link, but I think it is better to understand the more general case, and simplify only if possible.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="MLE-for-GLMs">MLE for GLMs<a class="anchor-link" href="#MLE-for-GLMs">&#182;</a></h2><p>We are tasked with finding the regression coefficients that maximize the likelihood of the data. So, first we need to define the likelihood function, then we need to see how our regression coefficients $\vec{\beta}$ influence that function. Finally we will want to find the values for the $\vec{\beta}$ vector that maximize the likelihood function.</p>
<p>Since we assume that our outcome variable is drawn from an exponential family distribution, we know that the probability density function for the distribution is given by:</p>
<p>$$
f\left(y_i|\theta_i, \phi, w_i\right) = e^{\frac{y_i\theta_i - b(\theta_i)}{\phi/w_i} - c(y_i, \phi)}
$$</p>
<p>Following the same logic used for linear regression, we can find the joint density for all the $y_i$ as:</p>
<p>$$
\mathcal{L}(\vec{\theta}|\vec{y},X) = \prod_{i=1}^{N} e^{\frac{y_i\theta_i - b(\theta_i)}{\phi/w_i} - c(y_i, \phi)}
$$</p>
<p>The log-likelihood function is:</p>
<p>$$
\mathcal{l}(\vec{\theta}|\vec{y},X) = \sum_{i=1}^{N} \frac{y_i\theta_i - b(\theta_i)}{\phi/w_i} - c(y_i, \phi)
$$</p>
<p>Now we have a function of $\vec{\theta}$ that we would like to maximize with respect to $\vec{\theta}$ (we will connect this to $\vec{\beta}$ as we go). Pulling out our calculus I skills, we can take the derivative of the function and set equal to zero. The derivative of the log-likelihood, $\partial{\mathcal{l}}/\partial{\theta}$ is often referred to as the "score" and will be denoted as $U$. Taking the score and setting equal to zero, we have (assume the $w_i = 1$):</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$$
\vec{U}(\vec{\beta}) = \frac{\partial \mathcal{l}}{\partial \vec{\beta}} = \frac{\partial \vec{\theta}}{\partial \vec{\beta}} \cdot \frac{\partial \mathcal{l}}{\partial \vec{\theta}}\\
= \vec{0}
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Expanding the second term above with the definition of the likelihood function:</p>
<p>$$
\frac{\partial \mathcal{l}}{\partial \vec{\theta_j}} = \frac{\partial}{\partial \vec{\theta_i}} \left( \sum_{i=1}^{N} \frac{y_i\theta_i - b(\theta_i)}{\phi/w_i} - c(y_i, \phi) \right)\\ 
= \frac{y_j - b'(\theta_j)}{\phi}\\
= \frac{y_j - \mu_j}{\phi}
$$</p>
<p>$$
\frac{\partial \mathcal{l}}{\partial \vec{\theta}} = \frac{1}{\phi}
\begin{bmatrix}
    y_0 - \mu_0 \\
    y_1 - \mu_1 \\
    \vdots \\
    y_N - \mu_N 
\end{bmatrix} = \frac{1}{\phi} \left(\vec{y} - \vec{\mu}\right)
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Substituting the above for $\frac{\partial \mathcal{l}}{\partial \vec{\theta}}$ the score equation becomes:</p>
<p>$$
\vec{U}(\vec{\beta}) = \frac{1}{\phi} \left(\frac{\partial \vec{\theta}}{\partial \vec{\beta}}\right) \left[\vec{y} - \vec{\mu}\right] = \frac{1}{\phi} \left(\frac{\partial \vec{\theta}}{\partial \vec{\beta}}\right) \left[\vec{y} - g^{-1}(X \vec{\beta})\right] = \vec{0}
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We now have a concrete equation which we need to solve to find the optimal regression coefficients $\beta$. This is serious progress! Take a moment to soak in where we are and how we got here. There really is a lot to keep track of and the connection from one parameter to the other, as well as the meaning of each, can get lost easily. I find it useful to remember the problem at a high level, and make simple, logical deductions to arrive at this final form.</p>
<h2 id="Solving-for-the-coefficients">Solving for the coefficients<a class="anchor-link" href="#Solving-for-the-coefficients">&#182;</a></h2><p>It feels great to have gotten the problem into a form that we could potentially solve. Still, as you may have feared, we have a problem. We know that $\vec{\mu} = g^{-1}(X \vec{\beta})$ and that $g(\vec{\mu})$ may not be a linear function. If it is not, we lack a closed form solution to the score equation.</p>
<p>One common method of overcoming this issue is to first use a first order Taylor expansion of $\vec{\mu}$ as an approximation. Then, we will see that the score equation can be manipulated to resemble the form of a simple weighted least squares regression (an ordinary least squares with weights), which has a nice closed form solution. We can then use the method of <a href="https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares">iteratively reweighted least squares</a> to find incrementally better approximations to the regression coefficients. A follow-up post will provide a comprehensive overview of the derivation and application of IRLS, so hopefully the promise of Taylor expansions to come can tide you over until then.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Further-reading">Further reading<a class="anchor-link" href="#Further-reading">&#182;</a></h2><ul>
<li><a href="http://www2.maths.bris.ac.uk/~peter/papers/IRLS.pdf">Green's paper</a> - THE paper on IRLS. Warning: hard to digest.</li>
<li><a href="http://web.as.uky.edu/statistics/users/pbreheny/760/S13/notes.html">University of Kentucky BST 760</a> - Notes from Dr. Patrick Breheny's "Advanced Regression" course. Good intuition, but glosses over some finer details.</li>
<li><a href="http://data.princeton.edu/wws509/notes/a2.pdf">Princeton GLM theory</a> - GLM Notes from German Rodriguez. </li>
<li><a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf">Andrew Ng's CS229 Notes</a> - Andrew Ng. Enough said.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The generalized linear regression interface allows the specification of <a href="https://en.wikipedia.org/wiki/Generalized_linear_model">generalized linear
models (GLMs)</a>.
GLMs allow for flexible specification
of linear models which can be used for various types of problems including linear regression,
Poisson regression, logistic regression, etc...</p>
<p>When working with data that
has a relatively small number of features (&lt; 4096) a GLR can be used to solve linear models on the
driver node instead of running an optimization algorithm on the distributed dataset.</p>
<p>Contrasted with linear regression where the output is assumed to have a Gaussian
distribution, GLMs are specifications of linear models where the output may take on <em>any</em>
distribution from the [exponential family of distributions]
(<a href="https://en.wikipedia.org/wiki/Exponential_family">https://en.wikipedia.org/wiki/Exponential_family</a>). An exponential family distribution is any
probability distribution of the form</p>
<p>$$
f\left(y|\theta, \phi, w\right) = e^{\frac{y\theta - b(\theta)}{\phi/w} - c(y, \phi)}\\
Y_i \sim f\left(\cdot \vert \theta_i, \phi, w_i\right)
$$</p>
<p>where the parameter of interest $\theta_i$ is related to the expected value of the response variable
$\mu_i$ by</p>
<p>$$
\theta_i = h(\mu_i)
$$</p>
<p>A GLM finds the regression coefficients $\vec{\beta}$ which maximize the joint probability density
of the data, also known as the likelihood.</p>
<p>$$
\min_{\vec{\beta}} \mathcal{L}(\vec{\theta}\vert\vec{y},X) =
\prod_{i=1}^{N} e^{\frac{y_i\theta_i - b(\theta_i)}{\phi/w_i} - c(y_i, \phi)}
$$</p>
<p>where the parameter of interest $\theta_i$ is related to the regression coefficients $\vec{\beta}$
by</p>
<p>$$
\theta_i = h(g^{-1}(\vec{x_i} \cdot \vec{\beta}))
$$</p>

</div>
</div>
</div>
 


<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

  </div>
  <div class="tag-cloud">
    <p>
      <a href="https://sethah.github.io/tag/ml.html">ml</a>
      <a href="https://sethah.github.io/tag/python.html">python</a>
    </p>
  </div>




</article>

    <footer>
<p>&copy; Seth Hendrickson </p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p>    </footer>
  </main>





<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Seth's Blog ",
  "url" : "https://sethah.github.io",
  "image": "/images/profile.jpg",
  "description": ""
}
</script>
</body>
</html>